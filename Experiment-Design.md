Step-by-Step Experimental Design

Goal: Validate whether joint adaptive LoRA-rank + mixed-precision quantization outperforms (a) fixed-rank LoRA, (b) adaptive-rank only, and (c) quantized LoRA only, under MacBook-class hardware.

â¸»

0.  Prep Checklist

Item	Choice
Frameworks	ðŸ¤— Transformers 4.40+, PEFT 0.10+, bitsandbytes 0.43 (CPU-safe build)
Device	Apple-silicon MacBook (M-series) â€” use MPS for FP16 runs; 8-bit quant runs on CPU; 4-bit runs via tiny cloud GPU if local int4 kernels unavailable
Task	SST-2 sentiment (GLUE) and WikiText-2 LM (â‰ˆ 2 h each) â†’ gives both classification & LM metrics
Model	bert-base-uncased (110 M) for SST-2, gpt-2-small (117 M) for LM
Metrics	SST-2 accuracy, WikiText-2 valid perplexity + training wall-clock + peak RAM


â¸»

1.  Environment & Data

1.1 Create conda/venv; install libraries.
1.2 datasets load SST-2 train/valid (â‰ˆ 67 k tokens) and WikiText-2.
1.3 Tokenize with HF fast tokenizers; cache to disk.

â¸»

2.  Baseline Configurations

ID	Quantization	LoRA rank pattern	Trainable %
B-FP	16-bit (no quant)	Fixed r = 8 all layers	~0.15%
B-Q4	4-bit NF4 (QLoRA)	Fixed r = 8	~0.15%
B-Ada	16-bit	AdaLoRA (init r = 12 âžœ target r = 4)	adaptive (~0.08â€“0.12%)


â¸»

3.  Joint Adaptive + Precision Variants

ID	Quantization plan	Rank plan
Joint-1	All layers 4-bit	AdaLoRA as in B-Ada
Joint-2	Embedding, layer-0, layer-(L-1) 8-bit; rest 4-bit	AdaLoRA
Joint-3	Attention matrices 8-bit, FFN 4-bit	Manual rank pattern: attention r = 6, FFN r = 10

Rationale: Joint-2 tests â€œcritical layers higher precisionâ€; Joint-3 tests precision-rank complementarity.

â¸»

4.  Implementation Details

4.1 Load & quantize:

config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)
model = AutoModel.from_pretrained(â€¦, quantization_config=config)
model = prepare_model_for_kbit_training(model)  # PEFT helper

For Joint-2/3, set module.exq_quant_config.skip=True on chosen layers or reload them in 8-bit.

4.2 Attach LoRA / AdaLoRA:

lora_cfg = AdaLoraConfig(â€¦init_r=12, target_r=4)  # or LoraConfig(rank=8)
model = get_peft_model(model, lora_cfg)

4.3 Training hyper-params:
	â€¢	LR = 2 e-4, AdamW, batch = 8 (SST-2) / 4 (Wiki); 3 epochs.
	â€¢	Gradient Accum = 2 on CPU to keep RAM â‰¤ 16 GB.
	â€¢	Learning-rate warm-up 10 %; cosine decay.
	â€¢	Clip grad = 1.0.
	â€¢	Log loss, grad-norm every 50 steps.

â¸»

5.  Run Matrix

Task	B-FP	B-Q4	B-Ada	Joint-1	Joint-2	Joint-3
SST-2	âœ“	âœ“	âœ“	âœ“	âœ“	âœ“
Wikitext-2	âœ“	âœ“	âœ“	âœ“	âœ“	âœ“

(6 configs Ã— 2 tasks = 12 runs; each â‰ˆ 0.5 â€“ 1 h on Mac; schedule overnight if needed).

â¸»

6.  Logging & Monitoring
	â€¢	Use Weights & Biases or local CSV for: loss curve, RAM usage (psutil), wall-clock.
	â€¢	Save best checkpoint (lowest valid loss/ highest acc).
	â€¢	Collect #trainable params via model.num_parameters(only_trainable=True).

â¸»

7.  Evaluation & Analysis

7.1 Compute metrics:

ppl = math.exp(eval_loss)
acc = sklearn.metrics.accuracy_score(labels, preds)

7.2 Create plots:
	â€¢	Accuracy / PPL vs effective model size (bytes).
	â€¢	Rank allocation heat map (AdaLoRA trimmed ranks).
	â€¢	Bar chart of peak RAM per run.

7.3 Stat tests: McNemar (SST-2) or paired t-test (ppl) to confirm joint variants â‰  baselines (Î±=0.05).

â¸»

8.  Ablation / Diagnostics (optional, if time)
	â€¢	Freeze AdaLoRA pruning âžœ fixed non-uniform rank, see if performance drops.
	â€¢	Swap which layers get 8-bit in Joint-2.
	â€¢	Try 3-bit (if hardware & bitsandbytes version allow) on the less-critical layers.

â¸»

9.  Expected Outcomes
	â€¢	Q4 alone â‰ˆ FP16 (minor loss).
	â€¢	AdaLoRA alone > fixed-rank FP16 (same params).
	â€¢	Joint-2 hypothesized best efficiency-performance (keeps critical layers precise + adaptive rank).
	â€¢	Memory footprint: Joint configs â‰ˆ Q4 baseline; trainable % â‰ˆ AdaLoRA (<0.15%).
	â€¢	Training speed: Q4 runs slower/CPU-bound; FP16 fastest on MPS; note in results.

â¸»

10.  Paper Artifacts
	â€¢	Table 1: Metric comparison across configs.
	â€¢	Figure 1: Pareto plot (bytes vs performance).
	â€¢	Appendix A: Rank allocations produced by AdaLoRA.
	â€¢	Appendix B: Mixed-precision map (layer â†’ bit-width).
	â€¢	Code + README: joint_adapt_rank_precision/.
